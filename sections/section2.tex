\section{Exercise 2}
\label{sec:sec2}
\textbf{Penalty parameter choice by minimizing the mean square error}
\begin{itemize}
  \item Define the parameterized mean square error function $F(\alpha):=\mathbb{E}\left[\left\|\hat{\beta}_{n}^{R}-\beta_{0}\right\|_{2}^{2}\right]$, where $\hat{\beta}_{n}^{R}$ is the ridge estimator with penalty parameter $\lambda_{n}=\alpha \times n^{1 / 3}$. Using similar simulation specifications as above produce a plot of the function $F$ for values of $\alpha$ between 0.01 and 0.1.
  \item Let us now define a new function $G$ as follows:
\end{itemize}

$$
G(\alpha):=\mathbb{E}\left[\operatorname{Trace}\left(\boldsymbol{W}_{n}\left(\alpha n^{1 / 3}\right)\left(\frac{\sigma^{2}}{n} \boldsymbol{Q}_{n}^{-1}+\alpha^{2} n^{2 / 3} \boldsymbol{Q}_{n}^{-1} \beta_{0} \beta_{0}{ }^{\prime} \boldsymbol{Q}_{n}^{-1}\right) \boldsymbol{W}_{n}\left(\alpha n^{1 / 3}\right)\right)\right]
$$

Compute by simulation function $G$, plot it and show that it is equal to the mean square function $F$. Explain your finding using the Ridge estimation theory developed in the course.

\begin{itemize}
  \item Substitute the parameters $\beta_{0}$ and $\sigma^{2}$ in function $G$ with the OLS estimator $\hat{\beta}_{n}^{O L S}$ and the error variance estimator $\hat{\sigma}_{n}^{2}$, in order to optimize the square error in each simulation by choosing the\\
optimal penalty parameter. For this purpose, in each simulation compute:
\end{itemize}

$$
\hat{\lambda}_{n}^{o p t}=\arg \min _{\lambda \geq 0} \operatorname{Trace}\left(W_{n}(\lambda)\left(\frac{\hat{\sigma}_{n}^{2}}{n} \boldsymbol{Q}_{n}^{-1}+\lambda^{2} \boldsymbol{Q}_{n}^{-1} \hat{\beta}_{n}^{O L S} \hat{\beta}_{n}^{O L S^{\prime}} \boldsymbol{Q}_{n}^{-1}\right) W_{n}(\lambda)\right)
$$

where

$$
\hat{\sigma}_{n}^{2}=\frac{\left\|\boldsymbol{Y}-\boldsymbol{X} \hat{\beta}_{n}^{O L S}\right\|_{2}^{2}}{n-K}
$$

Using such optimal penalty parameter compute the Ridge estimator and show that it delivers a mean square error similar to the minimum of functions $F$ and $G$.