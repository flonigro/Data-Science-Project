\section{Exercise 1}
Given a linear model $Y_{i}=X_{i} \beta_{0}+\epsilon_{i}$, with $X_{i} \in \mathbb{R}^{K}$, the ridge estimator is defined as follows


\begin{equation}
\hat{\beta}_{n}^{R}:=\arg \min _{\beta \in \mathbb{R}^{k}}\left\{\frac{1}{2 n}\|\boldsymbol{Y}-\boldsymbol{X} \beta\|_{2}^{2}+\frac{\lambda_{n}}{2}\|\beta\|_{2}^{2}\right\} \label{eq:beta-Hat}
\end{equation}


where $n$ is the number of observations and $\lambda_{n} \in \mathbb{R}_{+}$is a penalty parameter.

\begin{enumerate}
  \item Monte Carlo simulation of OLS and Ridge estimators: Simulate observations for $\left\{X_{i}, Y_{i}\right\}$ following the linear model with specifications given by $X_{i} \sim N\left(0, I_{K}\right), \epsilon \sim N\left(0, \sigma^{2} I_{n}\right), \sigma^{2}=.5, n=50$, $K=5$ and $\beta_{0}=[0.1, .05,0.2,0.9,0.5]$.
\end{enumerate}

\begin{itemize}
  \item For various simulations compute the OLS and Ridge estimators, $\hat{\beta}_{n}^{O L S}$ and $\hat{\beta}_{n}^{R}$, for penalty parameter $\lambda_{n}=0.1 \times n^{1 / 3}$.
  \item Show that the two estimators satisfy the following relation:
\end{itemize}

$$
\hat{\beta}_{n}^{R}=\boldsymbol{W}_{n}\left(\lambda_{n}\right) \hat{\beta}_{n}^{O L S}
$$

where $\boldsymbol{W}_{n}\left(\lambda_{n}\right)=\left(\boldsymbol{I}_{K}+\lambda_{n} \boldsymbol{Q}_{n}^{-1}\right)^{-1}, \boldsymbol{Q}_{n}=\boldsymbol{X}^{\prime} \boldsymbol{X} / n$ and $\boldsymbol{I}_{K}$ is a $K \times K$ identity matrix.

\begin{itemize}
  \item Plot the histogram of the OLS and the Ridge estimators in your simulation. What can you observe about the bias and variance of the estimators?
  \item Show that the Ridge estimator has lower variance than the OLS estimator, i.e.,
\end{itemize}

$$
\mathbb{V a r}\left(\hat{\beta}_{n}^{O L S} \mid \boldsymbol{X}\right) \succ \mathbb{V} a r\left(\hat{\beta}_{n}^{R} \mid \boldsymbol{X}\right)
$$

Hint: This is an inequality between matrices!
\label{sec:sec1}

